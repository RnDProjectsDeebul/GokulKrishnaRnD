{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91eec63c-aecc-4fc4-a180-c96d8cd1a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import yaml\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18, resnet50\n",
    "import torchvision.models.detection as detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fac1be2a-e950-4092-ba82-e043c76c2cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def load_class_info(yaml_file):\n",
    "    with open(yaml_file, 'r') as file:\n",
    "            class_info = yaml.safe_load(file)\n",
    "    return class_info['classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "002b2439-33e9-4fb9-a92e-522bc326c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalDataLoader(Dataset):\n",
    "    def __init__(self, dataset_folder, class_info_file, transform=None, demo=False):\n",
    "        self.dataset_folder = dataset_folder\n",
    "        self.transform = transform\n",
    "        # self.label_transform = label_transform\n",
    "        self.demo = demo\n",
    "        self.imgs_files = self.load_data(dataset_folder)\n",
    "        self.class_names = load_class_info(class_info_file)\n",
    "        self.num_classes = len(self.class_names)\n",
    "        # self.normalize_keypoints = normalize_keypoints\n",
    "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(self.class_names)}\n",
    "        print(self.class_to_idx)\n",
    "\n",
    "    def load_data(self, dataset_folder):\n",
    "        images_path = os.path.join(self.dataset_folder,\"images/\")\n",
    "        annotations_path = os.path.join(self.dataset_folder,\"annotations/\")\n",
    "        j_data = []\n",
    "        for file in os.listdir(images_path):\n",
    "            if file.endswith(\".jpg\"):\n",
    "                json_path = os.path.join(annotations_path, file.split('.')[0] + '.json')\n",
    "                with open(json_path) as f:\n",
    "                    json_load = json.load(f)\n",
    "                    for item in json_load['shapes']:\n",
    "                        points = [value for row in item['points'] for value in row]\n",
    "                        j_data.append({'image':  os.path.join(images_path,file),\n",
    "                                     'label': item['label'],\n",
    "                                     'points':points })\n",
    "        json_data = pd.DataFrame(j_data)\n",
    "        return json_data\n",
    "\n",
    "    def get_keypoint(self, bboxes):\n",
    "        centers = []\n",
    "        for bbox in bboxes:\n",
    "            center_x = (bbox[0] + bbox[2]) / 2\n",
    "            center_y = (bbox[1] + bbox[3]) / 2\n",
    "            centers.append((center_x, center_y))\n",
    "        return centers\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target = {}\n",
    "        img_path, label, bboxes_original = self.imgs_files.iloc[idx]\n",
    "        bboxes_original = [bboxes_original]\n",
    "        label = torch.tensor(self.class_to_idx[label], dtype=torch.int32)\n",
    "        keypoint_original = torch.tensor(self.get_keypoint(bboxes_original), dtype=torch.float32)\n",
    "        # img_original = cv2.imread(img_path)\n",
    "        # img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "        # img_original = read_image(img_path).float() / 255.0\n",
    "        img_original = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img_original = self.transform(img_original)\n",
    "\n",
    "        return img_original, keypoint_original, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01b7d46a-7447-46f0-9889-a4c746d5e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1841dd44-b045-4fde-88a1-76350acce037",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18Model(nn.Module):\n",
    "    def __init__(self, num_classes, num_keypoints=1):\n",
    "        super(ResNet18Model, self).__init__()\n",
    "        self.backbone = resnet18(pretrained=True)\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes + num_keypoints * 2) # 2 for each keypoint (x, y)\n",
    "        self.num_keypoints = num_keypoints\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(x)\n",
    "        keypoints = outputs[:, -self.num_keypoints * 2:]  # Last 2*num_keypoints values are keypoints\n",
    "        class_logits = outputs[:, :-self.num_keypoints * 2]  # The rest are class logits\n",
    "        return class_logits, keypoints\n",
    "\n",
    "class ResNet50Model(nn.Module):\n",
    "    def __init__(self, num_classes, num_keypoints=1):\n",
    "        super(ResNet50Model, self).__init__()\n",
    "        self.backbone = resnet50(pretrained=True)\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes + num_keypoints * 2) # 2 for each keypoint (x, y)\n",
    "        self.num_keypoints = num_keypoints\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(x)\n",
    "        keypoints = outputs[:, -self.num_keypoints * 2:]  # Last 2*num_keypoints values are keypoints\n",
    "        class_logits = outputs[:, :-self.num_keypoints * 2]  # The rest are class logits\n",
    "        return class_logits, keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10400d68-a5dd-43b2-8828-d6747d6d40f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LaplaceNLLLoss(input_data, target, scale, eps=1e-06, reduction='mean'):\n",
    "    device = input_data.device\n",
    "    target = target.to(device)\n",
    "    scale = scale.to(device)\n",
    "    \n",
    "    # loss = torch.log(2*scale) + torch.abs(input_data - target)/scale\n",
    "\n",
    "    # Inputs and targets much have same shape\n",
    "    input_data = input_data.view(input_data.size(0), -1)\n",
    "    target = target.view(target.size(0), -1)\n",
    "    if input_data.size() != target.size():\n",
    "        raise ValueError(\"input and target must have same size\")\n",
    "\n",
    "    # Second dim of scale must match that of input or be equal to 1\n",
    "    scale = scale.view(input_data.size(0), -1)\n",
    "    if scale.size(1) != input_data.size(1) and scale.size(1) != 1:\n",
    "        raise ValueError(\"scale is of incorrect size\")\n",
    "\n",
    "    # Check validity of reduction mode\n",
    "    if reduction != 'none' and reduction != 'mean' and reduction != 'sum':\n",
    "        raise ValueError(reduction + \" is not valid\")\n",
    "        \n",
    "    # Entries of var must be non-negative\n",
    "    if torch.any(scale < 0):\n",
    "        raise ValueError(\"scale has negative entry/entries\")\n",
    "\n",
    "    # Clamp for stability\n",
    "    scale = scale.clone()\n",
    "    with torch.no_grad():\n",
    "        scale.clamp_(min=eps)\n",
    "\n",
    "    # Calculate loss (without constant)\n",
    "    loss = (torch.log(2*scale) + torch.abs(input_data - target) / scale).view(input_data.size(0), -1).sum(dim=1)\n",
    "\n",
    "\n",
    "    # Apply reduction\n",
    "    if reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    else:\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1dcd3b2e-7964-47cf-a50e-f3f97872545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, dataloader, criterion_cls, criterion_kpt, optimizer, num_epochs=10):\n",
    "#     train_losses = []\n",
    "#     key_train_losses = []\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         total_train_loss = 0.0\n",
    "#         total_kpt_loss = 0.0\n",
    "\n",
    "#         for inputs, keypoints, labels in dataloader:\n",
    "#             optimizer.zero_grad()\n",
    "#             class_logits, predicted_keypoints = model(inputs)\n",
    "#             loss_classification = criterion_cls(class_logits, labels)\n",
    "#             loss_keypoints = criterion_kpt(predicted_keypoints, keypoints)\n",
    "#             loss_cls = loss_classification \n",
    "#             loss_kpt = loss_keypoints\n",
    "#             loss_cls.backward(retain_graph=True)\n",
    "#             loss_kpt.backward()\n",
    "#             optimizer.step()\n",
    "#             total_train_loss += loss_cls.item()\n",
    "#             total_kpt_loss += loss_kpt.item()\n",
    "\n",
    "#         avg_train_loss = total_train_loss / len(dataloader.dataset)\n",
    "#         avg_kpt_loss = total_kpt_loss / len(dataloader.dataset)\n",
    "        \n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss}, Train Keypoint Loss: {avg_kpt_loss}')\n",
    "        \n",
    "#         train_losses.append(avg_train_loss)\n",
    "#         key_train_losses.append(avg_kpt_loss)\n",
    "\n",
    "#     return model, train_losses, key_train_losses\n",
    "\n",
    "def train_model(device, model, criterion_cls, criterion_kpt, optimizer, train_loader, valid_loader, num_epochs=25):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    key_train_losses = []\n",
    "    key_valid_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        total_kpt_loss = 0.0\n",
    "\n",
    "        for images, keypoints, labels in train_loader:\n",
    "            labels = torch.stack([label.to(device) for label in labels])\n",
    "            images = torch.stack(images).to(device)\n",
    "            ground_truth_keypoints = torch.stack([keypoint.to(device) for keypoint in keypoints])\n",
    "\n",
    "            # labels_tensor = torch.stack(labels)\n",
    "            print(labels)\n",
    "            class_indices = labels\n",
    "            # print('labels shape: ', labels.count)\n",
    "            optimizer.zero_grad()\n",
    "            # print(abc)\n",
    "\n",
    "            class_outputs, keypoint_outputs = model(images)\n",
    "            print(class_outputs)\n",
    "            scale = torch.rand(keypoint_outputs.shape) + 1e-6\n",
    "            loss_cls = criterion_cls(class_outputs, class_indices)\n",
    "            # loss_kpt=None\n",
    "            loss_kpt = LaplaceNLLLoss(keypoint_outputs, ground_truth_keypoints, scale)\n",
    "                # loss_kpt = criterion_kpt(keypoint_outputs, ground_truth_keypoints)\n",
    "            class_loss = loss_cls \n",
    "            kpt_loss = loss_kpt\n",
    "            # total_loss = loss_cls + loss_kpt # Combine losses for backpropagation\n",
    "            class_loss.backward(retain_graph=True)\n",
    "            kpt_loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_train_loss += loss_cls.item()\n",
    "            total_kpt_loss += loss_kpt.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "        avg_kpt_loss = total_kpt_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss}, Train Keypoint Loss: {avg_kpt_loss}')\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        total_val_kpt_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, keypoints, labels in valid_loader:\n",
    "                labels = torch.stack([label.to(device) for label in labels])\n",
    "                images = torch.stack(images).to(device)\n",
    "                ground_truth_keypoints = torch.stack([keypoint.to(device) for keypoint in keypoints])\n",
    "\n",
    "                labels_tensor = torch.stack(labels)\n",
    "                class_indices = torch.argmax(labels_tensor, dim=1)\n",
    "\n",
    "                class_outputs, keypoint_outputs = model(images)\n",
    "                \n",
    "                scale = torch.rand(keypoint_outputs.shape) + 1e-6\n",
    "                loss_cls = criterion_cls(class_outputs, class_indices)\n",
    "                loss_kpt = LaplaceNLLLoss(keypoint_outputs, ground_truth_keypoints, scale)\n",
    "                    # loss_kpt = criterion_kpt(keypoint_outputs, ground_truth_keypoints)\n",
    "                \n",
    "                total_val_loss += loss_cls.item() \n",
    "                total_val_kpt_loss += loss_kpt.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(valid_loader.dataset)\n",
    "        avg_val_kpt_loss = total_val_kpt_loss / len(valid_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss}, Val Keypoint Loss: {avg_val_kpt_loss}')\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        valid_losses.append(avg_val_loss)\n",
    "        key_train_losses.append(avg_kpt_loss)\n",
    "        key_valid_losses.append(avg_val_kpt_loss)\n",
    "\n",
    "    return model, train_losses, key_train_losses, valid_losses, key_valid_losses  # Optionally return the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d4530044-6a82-4c7b-a8fa-83689a24a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ssd(model, dataloader, criterion_cls, criterion_kpt, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, keypoints, labels in dataloader:\n",
    "            targets = [{'boxes': keypoints, 'labels': labels}]\n",
    "            loss_dict = model(inputs, targets)\n",
    "            classification_loss = criterion_cls(loss_dict['cls_logits'], labels)\n",
    "            keypoints_loss = criterion_kpt(loss_dict['boxes'], keypoints)\n",
    "            loss_cls = classification_loss\n",
    "            loss_kpt = keypoints_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss_cls.backward(retain_graph=True)\n",
    "            loss_kpt.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += (loss_cls.item() + loss_kpt.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "64fd216d-f962-48be-8d17-b73bf24e74fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots_all(loss, epochs, label_loss, path):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(loss, label=label_loss)\n",
    "    plt.title(label_loss+' Over'+str(epochs)+' Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3d62b8c6-f3bc-4fda-aa9e-618acb9f772d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AllenKey': 0, 'Axis2': 1, 'Bearing2': 2, 'Drill': 3, 'F20_20_B': 4, 'F20_20_G': 5, 'Housing': 6, 'M20': 7, 'M20_100': 8, 'M30': 9, 'Motor2': 10, 'S40_40_B': 11, 'S40_40_G': 12, 'Screwdriver': 13, 'Spacer': 14, 'Wrench': 15, 'container_blue': 16, 'container_red': 17}\n",
      "{'AllenKey': 0, 'Axis2': 1, 'Bearing2': 2, 'Drill': 3, 'F20_20_B': 4, 'F20_20_G': 5, 'Housing': 6, 'M20': 7, 'M20_100': 8, 'M30': 9, 'Motor2': 10, 'S40_40_B': 11, 'S40_40_G': 12, 'Screwdriver': 13, 'Spacer': 14, 'Wrench': 15, 'container_blue': 16, 'container_red': 17}\n"
     ]
    }
   ],
   "source": [
    "class_config_path = './../config/formated_class.yaml'\n",
    "DATASET_FOLDER_TRAIN = './../../../RnD_datasets/robocup_dataset'\n",
    "train_path = os.path.join(DATASET_FOLDER_TRAIN,\"train/\")\n",
    "val_path = os.path.join(DATASET_FOLDER_TRAIN,\"val/\")\n",
    "\n",
    "num_classes = len(load_class_info(class_config_path))\n",
    "\n",
    "train_dataset = NormalDataLoader(train_path, class_config_path, transform=transform, demo=True)\n",
    "val_dataset = NormalDataLoader(val_path, class_config_path, transform=transform, demo=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.9, weight_decay=0.0005) #SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "085e0190-0d66-4a1c-9480-6b70e320c364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2, 12,  6,  4, 13,  6, 11,  0, 11, 12,  5,  5,  0,  2, 14, 12,  5, 12,\n",
      "         3, 12,  4,  2,  4, 12,  9, 11, 13,  5, 10,  4, 14,  0],\n",
      "       device='cuda:0', dtype=torch.int32)\n",
      "tensor([[-1.3732e-02, -2.1905e-01, -2.3761e-02, -3.7269e-01, -5.4984e-01,\n",
      "         -9.0506e-01, -2.3215e-01,  2.0684e-02,  3.3189e-01, -1.7693e-01,\n",
      "         -1.9543e-01, -2.9541e-01, -4.2110e-01,  6.5649e-01, -7.3906e-01,\n",
      "         -5.0231e-02,  1.4081e-01,  8.2963e-01],\n",
      "        [ 6.4222e-01, -1.5526e-01, -1.0694e+00,  1.4538e-01, -5.2045e-01,\n",
      "         -5.0094e-01, -1.2113e-01, -4.4936e-02,  3.4638e-01,  2.9641e-01,\n",
      "          2.3925e-01, -8.4414e-01,  8.9996e-02,  4.5097e-01, -6.5466e-01,\n",
      "          2.6347e-01, -4.8234e-01,  8.3424e-01],\n",
      "        [ 1.0573e+00, -6.9146e-01,  2.7343e-02, -4.6398e-03,  1.6400e-01,\n",
      "          2.6010e-02,  2.5012e-01, -1.0207e+00, -5.1289e-04,  4.4597e-01,\n",
      "          6.8850e-01, -9.4987e-01, -2.8960e-01,  1.2757e+00, -9.2715e-01,\n",
      "          4.1576e-01, -1.2474e-01,  1.5011e+00],\n",
      "        [-2.1485e-01, -3.9047e-01, -8.5052e-01, -2.9038e-01, -5.2238e-01,\n",
      "         -1.3641e+00, -1.4440e-01,  2.7506e-01,  4.8217e-01,  7.0813e-01,\n",
      "          2.2121e-01, -6.9484e-01,  4.4679e-02,  7.8198e-01, -9.7678e-01,\n",
      "          6.2010e-02, -4.2985e-01,  8.3184e-01],\n",
      "        [ 1.6002e-01, -1.0666e-02, -7.7529e-01,  6.5496e-03, -3.9070e-01,\n",
      "         -1.2336e+00,  7.4498e-03,  6.3220e-02,  1.0630e+00,  2.0629e-01,\n",
      "         -1.8855e-01, -4.2179e-01, -2.2237e-01,  8.6830e-01, -2.6573e-01,\n",
      "          2.0709e-01, -5.5973e-01,  5.8193e-01],\n",
      "        [ 9.5388e-01,  9.7744e-02, -6.1190e-01,  1.6910e-02, -5.7673e-01,\n",
      "         -2.7061e-01, -1.7372e-01, -2.4265e-01,  1.1282e-01,  4.3042e-02,\n",
      "         -2.2276e-01, -6.4205e-01,  4.8000e-01,  7.7481e-01, -6.3078e-01,\n",
      "          4.2439e-01, -5.9998e-01,  3.3346e-01],\n",
      "        [ 1.0573e+00, -6.9146e-01,  2.7343e-02, -4.6398e-03,  1.6400e-01,\n",
      "          2.6010e-02,  2.5012e-01, -1.0207e+00, -5.1289e-04,  4.4597e-01,\n",
      "          6.8850e-01, -9.4987e-01, -2.8960e-01,  1.2757e+00, -9.2715e-01,\n",
      "          4.1576e-01, -1.2474e-01,  1.5011e+00],\n",
      "        [ 2.1550e-01,  2.4427e-01, -3.4503e-01,  2.8377e-01, -4.3559e-01,\n",
      "         -4.2384e-01,  4.6014e-01, -5.1304e-01, -5.4549e-01,  1.3890e-02,\n",
      "         -6.5922e-01, -8.7642e-01,  4.6007e-01,  1.5748e+00, -1.4431e+00,\n",
      "          1.2474e+00, -1.1423e+00,  6.5794e-01],\n",
      "        [ 3.2896e-01, -2.4084e-01, -9.6348e-01,  5.2139e-02, -4.9318e-01,\n",
      "         -7.6970e-01,  4.6275e-01, -2.9998e-01,  3.3360e-02,  2.5136e-01,\n",
      "          4.9675e-03, -7.7038e-01, -1.3353e-01,  7.6721e-01, -4.9677e-01,\n",
      "          5.8798e-01, -1.6557e-01,  1.4483e+00],\n",
      "        [ 4.7851e-01, -1.1236e-02, -1.7872e-01,  2.3311e-01, -4.0982e-01,\n",
      "         -1.4004e-01, -3.5120e-01,  3.1083e-01,  7.2007e-02,  1.4748e-01,\n",
      "          3.3057e-01, -8.2258e-01,  1.2031e-01,  9.9470e-01, -1.4373e-01,\n",
      "          5.3419e-01, -3.9996e-01,  6.1229e-01],\n",
      "        [ 9.6978e-01, -1.4763e+00, -1.1105e-01, -2.4787e-01, -4.3008e-01,\n",
      "         -2.5985e-01,  3.5223e-01, -6.3035e-01, -1.7473e-01,  1.1121e-01,\n",
      "          4.5860e-01, -1.2593e+00, -3.8845e-01,  1.4896e+00, -1.0301e+00,\n",
      "          3.2400e-01, -9.6884e-02,  1.4988e+00],\n",
      "        [ 3.8151e-01,  2.1529e-02, -2.3530e-01, -2.6509e-01, -1.0631e-01,\n",
      "         -3.4222e-01,  1.7835e-01, -1.1494e-01,  4.0203e-01,  2.2849e-01,\n",
      "         -1.2660e-01, -4.9983e-01,  5.5297e-01,  1.3407e+00, -1.2145e+00,\n",
      "          5.1136e-01, -3.9182e-01,  3.6564e-01],\n",
      "        [ 1.8151e-01,  2.9347e-02, -6.2874e-01,  3.0902e-02, -2.8907e-01,\n",
      "         -4.9205e-01,  6.5362e-02, -3.4754e-01,  2.6740e-01,  3.3587e-01,\n",
      "          4.4765e-01, -4.2077e-01,  5.4150e-02,  6.6061e-01, -7.3573e-01,\n",
      "          7.5795e-02,  2.3764e-01,  6.4825e-01],\n",
      "        [ 2.4750e-01, -9.5026e-01, -1.5736e-01,  4.8590e-01,  2.2213e-01,\n",
      "         -8.4528e-01,  9.0665e-01, -1.9891e-01, -3.0741e-01,  7.7675e-02,\n",
      "          5.8867e-01, -4.0972e-01,  5.0355e-01,  1.5758e+00, -1.0422e+00,\n",
      "          5.8469e-01, -7.7442e-01,  8.4401e-02],\n",
      "        [-1.2601e-02, -8.1569e-01,  3.9378e-01, -7.8378e-02, -6.0347e-01,\n",
      "         -8.5041e-01, -7.3375e-02,  2.1479e-01,  2.3207e-02, -5.8867e-01,\n",
      "         -3.5059e-01, -1.1119e+00,  4.7261e-01,  1.1945e+00, -4.0495e-01,\n",
      "          3.4246e-01, -5.0633e-01,  7.3039e-01],\n",
      "        [ 2.2396e-01,  1.4497e-01, -1.3343e-01,  2.1980e-01, -3.6404e-01,\n",
      "         -6.0213e-01, -6.4473e-02, -2.5588e-01,  7.7751e-02,  1.7182e-01,\n",
      "          2.6474e-01, -1.2558e+00,  3.6466e-01,  6.4655e-01, -5.0043e-01,\n",
      "          1.2496e-01, -5.1893e-01,  7.0359e-01],\n",
      "        [ 4.5331e-01, -5.2109e-01, -7.0300e-01, -9.6699e-02, -2.3331e-01,\n",
      "          1.6105e-02, -2.6959e-01, -6.5324e-02,  1.0891e+00,  2.1773e-01,\n",
      "         -2.0563e-01, -1.3038e-01, -4.9892e-01,  2.1212e+00, -3.9662e-01,\n",
      "          5.5911e-01, -7.4924e-01,  6.7077e-01],\n",
      "        [ 1.0573e+00, -6.9146e-01,  2.7343e-02, -4.6398e-03,  1.6400e-01,\n",
      "          2.6010e-02,  2.5012e-01, -1.0207e+00, -5.1289e-04,  4.4597e-01,\n",
      "          6.8850e-01, -9.4987e-01, -2.8960e-01,  1.2757e+00, -9.2715e-01,\n",
      "          4.1576e-01, -1.2474e-01,  1.5011e+00],\n",
      "        [ 5.2438e-01, -3.4731e-01,  1.2071e-01,  6.3542e-01, -3.0711e-01,\n",
      "          1.2260e-01,  9.6553e-01, -2.5801e-01, -3.6182e-01, -3.0545e-01,\n",
      "         -1.1544e-01, -3.4238e-01,  5.9785e-01,  1.2635e+00, -1.0995e+00,\n",
      "          1.2986e-01, -2.4588e-01,  8.0763e-01],\n",
      "        [ 5.0581e-01, -2.5570e-01, -1.3299e-01,  3.7187e-01, -3.1967e-03,\n",
      "         -7.0205e-02, -4.0875e-01, -4.2525e-01,  4.2381e-01,  1.0542e-01,\n",
      "          5.2707e-02, -1.2255e+00,  3.3151e-01,  1.2260e+00, -1.1353e+00,\n",
      "          8.1325e-01, -4.9457e-01,  1.3042e-01],\n",
      "        [ 1.8624e-01, -1.2774e-03, -2.9635e-01, -6.4570e-02, -7.2604e-01,\n",
      "         -4.1945e-01,  1.1881e-01, -2.4020e-01,  2.6657e-01,  7.9366e-03,\n",
      "         -1.5848e-03, -3.8624e-01, -2.7218e-01,  6.7615e-01, -3.9401e-01,\n",
      "          8.7952e-02,  1.9369e-01,  1.2824e+00],\n",
      "        [ 9.9256e-01,  2.6946e-01,  3.5730e-01,  6.9877e-01, -7.3692e-01,\n",
      "         -9.0829e-01, -3.9285e-01, -4.7961e-01,  9.1845e-01,  3.4743e-01,\n",
      "         -2.5368e-01, -5.0467e-01,  4.8116e-01,  1.6278e+00, -8.9507e-01,\n",
      "          7.3061e-01, -8.9965e-01,  8.3255e-01],\n",
      "        [-7.5572e-02, -5.1917e-01, -5.3284e-02,  2.4750e-01, -4.3503e-02,\n",
      "         -5.7463e-01,  4.8216e-01,  3.7438e-02,  6.9522e-01, -9.0715e-02,\n",
      "          8.1580e-02, -1.0392e+00, -2.5134e-01,  1.1558e+00, -8.3164e-01,\n",
      "          1.0647e-01, -5.7809e-01,  5.0850e-02],\n",
      "        [-3.7501e-01, -2.5404e-01, -1.2741e-01, -4.8180e-01, -2.7749e-01,\n",
      "         -6.4972e-01,  3.6407e-01, -4.7715e-01,  5.8868e-01,  2.9668e-02,\n",
      "          3.6671e-01, -3.7813e-01,  2.8761e-01,  9.0208e-01, -7.1504e-01,\n",
      "          4.9166e-01, -8.7410e-01,  6.7307e-01],\n",
      "        [-1.7496e-02, -1.8137e-01, -5.0076e-02, -3.2160e-01,  2.1791e-01,\n",
      "         -6.6644e-01,  1.0830e+00, -7.3559e-01,  4.3030e-01, -6.1880e-01,\n",
      "         -7.5653e-02, -8.0994e-01,  1.1124e-01,  1.5797e+00, -7.3780e-01,\n",
      "          2.1726e-01,  2.2511e-02,  1.5649e+00],\n",
      "        [ 4.1676e-01, -6.8453e-01,  3.9029e-01,  1.5931e-02, -2.0792e-01,\n",
      "         -8.5575e-01,  3.5818e-02,  1.3162e-01,  3.4959e-01,  2.4718e-01,\n",
      "          1.0073e-02, -2.6819e-01,  2.1885e-01,  1.2141e+00, -1.0347e+00,\n",
      "          6.1513e-01, -5.5614e-01,  1.1256e+00],\n",
      "        [ 5.4482e-01, -1.2701e-01, -5.3505e-01,  3.2289e-01, -2.2435e-01,\n",
      "         -7.1176e-01,  2.5345e-02,  9.3190e-02,  8.0073e-02,  1.7501e-01,\n",
      "          2.6588e-01, -9.6746e-01,  2.9553e-02,  8.4261e-01, -1.0949e-01,\n",
      "          3.3949e-01,  4.4158e-02,  7.6343e-02],\n",
      "        [ 3.7990e-01, -1.2157e-02, -4.6782e-01,  8.4726e-02, -4.2809e-01,\n",
      "         -8.4415e-01, -4.1569e-02, -2.0885e-01, -2.0139e-01,  3.7467e-01,\n",
      "          2.7031e-01, -6.5633e-01, -8.2310e-02,  1.0359e+00, -7.5236e-01,\n",
      "          5.5745e-01,  2.0704e-01,  1.0299e+00],\n",
      "        [ 1.8265e-01, -8.2225e-01, -6.6957e-01,  1.9525e-01,  1.1283e-01,\n",
      "         -9.0614e-01,  5.3606e-01, -6.3115e-01, -4.5253e-01,  1.6736e-01,\n",
      "         -1.3306e-01, -3.8759e-01,  1.5127e-01,  6.6809e-01, -4.1095e-01,\n",
      "          4.5938e-01, -7.6473e-02,  1.1096e+00],\n",
      "        [ 3.1725e-01, -3.3820e-01,  1.4862e-01,  4.1986e-02, -3.1519e-01,\n",
      "         -5.0422e-01,  2.0524e-01, -8.7892e-02, -1.6238e-01,  1.3784e-01,\n",
      "         -2.7357e-01,  2.2569e-02, -2.9761e-01,  1.2102e+00, -1.2215e+00,\n",
      "          1.8170e-01, -4.7293e-01,  9.6286e-01],\n",
      "        [ 6.3029e-01, -1.4114e+00,  1.3726e-02,  3.6322e-01,  2.1281e-01,\n",
      "         -7.3910e-02,  4.4192e-03, -1.1083e+00,  1.6852e-01, -7.2295e-01,\n",
      "          5.5496e-01, -7.2402e-01,  3.0725e-01,  1.6864e+00, -9.6758e-01,\n",
      "          7.9877e-01,  1.0884e-01,  1.4886e+00],\n",
      "        [ 4.9561e-01, -6.5706e-01, -5.1424e-01, -4.4378e-01, -6.6404e-02,\n",
      "          1.7292e-01, -5.8697e-01,  4.0504e-01,  1.1981e+00,  6.4622e-02,\n",
      "         -3.2932e-01,  8.7524e-02,  2.3230e-02,  2.1919e+00, -5.9060e-01,\n",
      "          4.7131e-01, -1.8030e-01,  1.2260e+00]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m criterion_keypoints \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m      8\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m---> 10\u001b[0m trained_model, train_loss, key_train_loss, valid_losses, key_valid_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_classification\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_keypoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./normal/models/resnet18_normal_100.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m plots_all(train_loss, epochs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./normal/plots/resnet18_normal_training_loss_100_epochs.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[74], line 58\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(device, model, criterion_cls, criterion_kpt, optimizer, train_loader, valid_loader, num_epochs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(class_outputs)\n\u001b[1;32m     57\u001b[0m scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(keypoint_outputs\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n\u001b[0;32m---> 58\u001b[0m loss_cls \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# loss_kpt=None\u001b[39;00m\n\u001b[1;32m     60\u001b[0m loss_kpt \u001b[38;5;241m=\u001b[39m LaplaceNLLLoss(keypoint_outputs, ground_truth_keypoints, scale)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Int'"
     ]
    }
   ],
   "source": [
    "model = ResNet18Model(num_classes=num_classes)\n",
    "model.to(device)\n",
    "\n",
    "criterion_classification = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion_keypoints = nn.MSELoss()\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "trained_model, train_loss, key_train_loss, valid_losses, key_valid_losses = train_model(device, model, criterion_classification, criterion_keypoints, optimizer, train_loader, valid_loader, num_epochs=epochs)\n",
    "\n",
    "torch.save(model.state_dict(), './normal/models/resnet18_normal_100.pth')\n",
    "\n",
    "plots_all(train_loss, epochs, 'Training Loss', './normal/plots/resnet18_normal_training_loss_100_epochs.png')\n",
    "plots_all(key_train_loss, epochs, 'Keypoint Training Loss', './normal/plots/resnet18_normal_keypoint_training_loss_100_epochs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e330fc-bf02-44f8-a09f-04f46b00430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50Model(num_classes=num_classes)\n",
    "model.to(device)\n",
    "\n",
    "criterion_classification = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion_keypoints = nn.MSELoss()\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "trained_model, train_loss, key_train_loss = train_model(model, dataloader, criterion_classification, criterion_keypoints, optimizer, num_epochs=epochs)\n",
    "\n",
    "torch.save(model.state_dict(), './normal/models/resnet50_normal_100.pth')\n",
    "\n",
    "plots_all(train_loss, epochs, 'Training Loss', './normal/plots/resnet50_normal_training_loss_100_epochs.png')\n",
    "plots_all(key_train_loss, epochs, 'Keypoint Training Loss', './normal/plots/resnet50_normal_keypoint_training_loss_100_epochs.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
