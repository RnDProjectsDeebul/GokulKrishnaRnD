{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "876b980b-9717-4e16-a7b1-68c166d3a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, cv2, numpy as np, matplotlib.pyplot as plt, yaml\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Func\n",
    "from datetime import datetime\n",
    "from torchvision.io import read_image\n",
    "import scipy.linalg\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2657550-1ade-4278-9de3-c03dcaf7ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_class_info(yaml_file):\n",
    "    with open(yaml_file, 'r') as file:\n",
    "            class_info = yaml.safe_load(file)\n",
    "    return class_info['classes']\n",
    "\n",
    "def label_transform(classes, labels, num_classes): #One-hot encoding\n",
    "    class_to_index = {class_name: index for index, class_name in enumerate(classes)}\n",
    "    target = np.zeros(num_classes, dtype=int)\n",
    "    for label in labels:\n",
    "        if label in class_to_index:\n",
    "            index = class_to_index[label]\n",
    "            target[index] = 1\n",
    "    return target\n",
    "\n",
    "def generate_ecc_codes(num_classes, code_length=None):\n",
    "    hadamard_size = 1\n",
    "    while hadamard_size < num_classes:\n",
    "        hadamard_size *= 2\n",
    "\n",
    "    # Generate the Hadamard matrix\n",
    "    M = scipy.linalg.hadamard(hadamard_size).astype(np.float32)\n",
    "    print(\"Scipy hadamard \", M.shape)\n",
    "    \n",
    "    # Replace the first column for every second row\n",
    "    M[np.arange(0, hadamard_size, 2), 0] = -1\n",
    "    print(\"Replaced \", M.shape)\n",
    "    \n",
    "    # Shuffle the rows and columns\n",
    "    np.random.seed(12754)\n",
    "    np.random.shuffle(M)\n",
    "    idx = np.random.permutation(hadamard_size)\n",
    "    \n",
    "    # Select the required number of rows and columns\n",
    "    M = M[0:num_classes, idx[0:code_length]]\n",
    "    \n",
    "    return M\n",
    "    \n",
    "def label_transform_ecc(classes, labels, codewords):\n",
    "    class_to_index = {class_name: index for index, class_name in enumerate(classes)}\n",
    "    \n",
    "    if not isinstance(codewords, (list, np.ndarray)):\n",
    "        raise ValueError(\"codewords should be a list or numpy array\")\n",
    "    \n",
    "    if len(codewords) == 0 or not isinstance(codewords[0], (list, np.ndarray)):\n",
    "        raise ValueError(\"codewords should be a non-empty list of lists or 2D numpy array\")\n",
    "    \n",
    "    codeword_length = len(codewords[0])\n",
    "    target = np.zeros(codeword_length, dtype=int)\n",
    "    \n",
    "    for label in labels:\n",
    "        if label in class_to_index:\n",
    "            index = class_to_index[label]\n",
    "            if index < len(codewords):\n",
    "                target = codewords[index]\n",
    "            else:\n",
    "                print(f\"Warning: Index {index} is out of range for codewords\")\n",
    "    \n",
    "    return target\n",
    "\n",
    "def ecc_encode(label, ecc_codes):\n",
    "    class_index = label.index(1)  # Assuming label is initially one-hot\n",
    "    return ecc_codes[class_index]\n",
    "    \n",
    "def transform(image, keypoints, one_hot_label, num_classes, image_size=(224, 224), fusion='concat'):\n",
    "    transform_ops = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "    ])\n",
    "    image = transform_ops(image)\n",
    "    keypoints = torch.tensor(keypoints).float()\n",
    "    one_hot_label = torch.tensor(one_hot_label).float()\n",
    "    one_hot_channel = one_hot_label.unsqueeze(1).unsqueeze(2).expand(len(one_hot_label), image_size[0], image_size[1])\n",
    "    one_hot_channel = one_hot_channel.sum(dim=0, keepdim=True)\n",
    "    \n",
    "    input_combined = torch.cat((image, one_hot_channel), dim=0)\n",
    "\n",
    "    if fusion == 'concat':\n",
    "        input_combined = torch.cat((image, one_hot_channel), dim=0)\n",
    "\n",
    "    elif fusion == 'add':\n",
    "        if image.shape[0] == 3:\n",
    "            padding = torch.zeros_like(image[0]).unsqueeze(0)\n",
    "            padded_one_hot_channel = torch.cat([one_hot_channel, padding, padding], dim=0)\n",
    "        input_combined = image + padded_one_hot_channel\n",
    "\n",
    "    elif fusion == 'multiply':\n",
    "        if image.shape[0] == 3:\n",
    "            one_hot_channel = one_hot_channel.repeat(3, 1, 1)\n",
    "        input_combined = image * one_hot_channel\n",
    "        \n",
    "    if fusion == 'concat':\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406, 0.0], std=[0.229, 0.224, 0.225, 1.0])\n",
    "    else:\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "    input_combined = normalize(input_combined)\n",
    "    \n",
    "    return input_combined, keypoints\n",
    "\n",
    "def denormalize_image(image):\n",
    "    denormalize = transforms.Normalize(\n",
    "        mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
    "        std=[1 / 0.229, 1 / 0.224, 1 / 0.225]\n",
    "    )\n",
    "    image = denormalize(image)\n",
    "    image = torch.clamp(image, 0, 1)\n",
    "    return image\n",
    "\n",
    "def visualize_keypoints(image, keypoints):\n",
    "    image = denormalize_image(image)\n",
    "    image = transforms.ToPILImage()(image)\n",
    "    plt.imshow(image)\n",
    "    plt.scatter(keypoints[:, 0], keypoints[:, 1], s=10, marker='.', c='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d48392c-c685-4f99-bb47-fb07a2c24a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoaderOneHot(Dataset):\n",
    "    def __init__(self, dataset_folder, class_info_file, transform=None, label_transform=None, fusion_type='concat'):\n",
    "        self.dataset_folder = dataset_folder\n",
    "        self.transform = transform\n",
    "        self.label_transform = label_transform\n",
    "        self.imgs_files = self.load_data(dataset_folder)\n",
    "        self.class_names = load_class_info(class_info_file)\n",
    "        self.num_classes = len(self.class_names)\n",
    "        self.fusion_type = fusion_type\n",
    "\n",
    "    def load_data(self, dataset_folder):\n",
    "        images_path = os.path.join(self.dataset_folder,\"images/\")\n",
    "        annotations_path = os.path.join(self.dataset_folder,\"annotations/\")\n",
    "        j_data = []\n",
    "        for file in os.listdir(images_path):\n",
    "            if file.endswith(\".jpg\"):\n",
    "                json_path = os.path.join(annotations_path, file.split('.')[0] + '.json')\n",
    "                with open(json_path) as f:\n",
    "                    json_load = json.load(f)\n",
    "                    for item in json_load['shapes']:\n",
    "                        points = [value for row in item['points'] for value in row]\n",
    "                        j_data.append({'image':  os.path.join(images_path,file),\n",
    "                                     'label': item['label'],\n",
    "                                     'points':points })\n",
    "        json_data = pd.DataFrame(j_data)\n",
    "        return json_data\n",
    "\n",
    "    def get_keypoint(self, bboxes):\n",
    "        centers = []\n",
    "        for bbox in bboxes:\n",
    "            center_x = (bbox[0] + bbox[2]) / 2\n",
    "            center_y = (bbox[1] + bbox[3]) / 2\n",
    "            centers.append((center_x, center_y))\n",
    "        return centers\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target = {}\n",
    "        img_path, label, bboxes_original = self.imgs_files.iloc[idx]\n",
    "        label = [label]\n",
    "        bboxes_original = [bboxes_original]\n",
    "        keypoint_original = self.get_keypoint(bboxes_original)\n",
    "        img_original = read_image(img_path).float() / 255.0\n",
    "\n",
    "        if self.transform:\n",
    "            label_transform = self.label_transform(self.class_names,label, self.num_classes)\n",
    "            img, target['keypoints'] = self.transform(img_original, keypoint_original, label_transform, self.num_classes, fusion=self.fusion_type)\n",
    "            target['labels'] = torch.as_tensor(label_transform, dtype=torch.int64)\n",
    "        else:\n",
    "            # img, bboxes = img_original, bboxes_original\n",
    "            img, target['keypoints'] = img_original, keypoint_original\n",
    "            target['labels'] = label\n",
    "\n",
    "        return img, target, img_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145a23b6-e682-42ec-8fe9-2b24c799414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoaderECC(Dataset):\n",
    "    def __init__(self, dataset_folder, class_info_file, fusion_type, transform=None, label_transform=None, codeWords=None, ):\n",
    "        self.dataset_folder = dataset_folder\n",
    "        self.transform = transform\n",
    "        self.label_transform = label_transform\n",
    "        self.imgs_files = self.load_data(dataset_folder)\n",
    "        self.class_names = load_class_info(class_info_file)\n",
    "        self.num_classes = len(self.class_names)\n",
    "        self.codeWords = codeWords\n",
    "        self.fusion_type = fusion_type\n",
    "        # print(self.fusion_type)\n",
    "\n",
    "    def load_data(self, dataset_folder):\n",
    "        images_path = os.path.join(self.dataset_folder,\"images/\")\n",
    "        annotations_path = os.path.join(self.dataset_folder,\"annotations/\")\n",
    "        j_data = []\n",
    "        for file in os.listdir(images_path):\n",
    "            if file.endswith(\".jpg\"):\n",
    "                json_path = os.path.join(annotations_path, file.split('.')[0] + '.json')\n",
    "                with open(json_path) as f:\n",
    "                    json_load = json.load(f)\n",
    "                    for item in json_load['shapes']:\n",
    "                        points = [value for row in item['points'] for value in row]\n",
    "                        j_data.append({'image':  os.path.join(images_path,file),\n",
    "                                     'label': item['label'],\n",
    "                                     'points':points })\n",
    "        json_data = pd.DataFrame(j_data)\n",
    "        return json_data\n",
    "\n",
    "    def get_keypoint(self, bboxes):\n",
    "        centers = []\n",
    "        for bbox in bboxes:\n",
    "            center_x = (bbox[0] + bbox[2]) / 2\n",
    "            center_y = (bbox[1] + bbox[3]) / 2\n",
    "            centers.append((center_x, center_y))\n",
    "        return centers\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target = {}\n",
    "        img_path, label, bboxes_original = self.imgs_files.iloc[idx]\n",
    "        label = [label]\n",
    "        bboxes_original = [bboxes_original]\n",
    "        keypoint_original = self.get_keypoint(bboxes_original)\n",
    "        img_original = read_image(img_path).float() / 255.0\n",
    "\n",
    "        if self.transform:\n",
    "            label_transform = self.label_transform(self.class_names,label, self.codeWords)\n",
    "            img, target['keypoints'] = self.transform(img_original, keypoint_original, label_transform, self.num_classes, fusion=self.fusion_type)\n",
    "            target['labels'] = torch.as_tensor(label_transform, dtype=torch.int64)\n",
    "        else:\n",
    "            # img, bboxes = img_original, bboxes_original\n",
    "            img, target['keypoints'] = img_original, keypoint_original\n",
    "            target['labels'] = label\n",
    "\n",
    "        return img, target, img_original\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fa0d7c6-918a-4964-9c1f-954dc8d0a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoaderNormal(Dataset):\n",
    "    def __init__(self, dataset_folder, class_info_file, transform=None, demo=False):\n",
    "        self.dataset_folder = dataset_folder\n",
    "        self.transform = transform\n",
    "        # self.label_transform = label_transform\n",
    "        self.demo = demo\n",
    "        self.imgs_files = self.load_data(dataset_folder)\n",
    "        self.class_names = load_class_info(class_info_file)\n",
    "        self.num_classes = len(self.class_names)\n",
    "        # self.normalize_keypoints = normalize_keypoints\n",
    "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(self.class_names)}\n",
    "\n",
    "    def load_data(self, dataset_folder):\n",
    "        images_path = os.path.join(self.dataset_folder,\"images/\")\n",
    "        annotations_path = os.path.join(self.dataset_folder,\"annotations/\")\n",
    "        j_data = []\n",
    "        for file in os.listdir(images_path):\n",
    "            if file.endswith(\".jpg\"):\n",
    "                json_path = os.path.join(annotations_path, file.split('.')[0] + '.json')\n",
    "                with open(json_path) as f:\n",
    "                    json_load = json.load(f)\n",
    "                    for item in json_load['shapes']:\n",
    "                        points = [value for row in item['points'] for value in row]\n",
    "                        j_data.append({'image':  os.path.join(images_path,file),\n",
    "                                     'label': item['label'],\n",
    "                                     'points':points })\n",
    "        json_data = pd.DataFrame(j_data)\n",
    "        return json_data\n",
    "\n",
    "    def get_keypoint(self, bboxes):\n",
    "        centers = []\n",
    "        for bbox in bboxes:\n",
    "            center_x = (bbox[0] + bbox[2]) / 2\n",
    "            center_y = (bbox[1] + bbox[3]) / 2\n",
    "            centers.append((center_x, center_y))\n",
    "        return centers\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target = {}\n",
    "        img_path, label, bboxes_original = self.imgs_files.iloc[idx]\n",
    "        bboxes_original = [bboxes_original]\n",
    "        label = torch.tensor(self.class_to_idx[label], dtype=torch.int32)\n",
    "        keypoint_original = torch.tensor(self.get_keypoint(bboxes_original), dtype=torch.float32)\n",
    "        img_original = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img_original = self.transform(img_original)\n",
    "\n",
    "        return img_original, keypoint_original, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "982b3235-0ba9-4b86-910e-b9f6981e5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=20, num_keypoints=1, num_channels=4): \n",
    "        super(CustomResNet18, self).__init__()\n",
    "        \n",
    "        self.num_keypoints = num_keypoints\n",
    "        self.resnet18 = models.resnet18(pretrained=True)\n",
    "        self.resnet18.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.fc_cls = nn.Linear(self.resnet18.fc.in_features, num_classes)\n",
    "        self.fc_kpts = nn.Linear(self.resnet18.fc.in_features, self.num_keypoints * 2)\n",
    "        self.resnet18.fc = nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.resnet18(x)\n",
    "        class_outputs = self.fc_cls(features)\n",
    "        keypoint_outputs = self.fc_kpts(features)\n",
    "        return class_outputs, keypoint_outputs.reshape(-1, self.num_keypoints, 2)\n",
    "\n",
    "\n",
    "class CustomResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=20, num_keypoints=1, num_channels=4): \n",
    "        super(CustomResNet50, self).__init__()\n",
    "        \n",
    "        self.num_keypoints = num_keypoints\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        self.resnet50.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.fc_cls = nn.Linear(self.resnet50.fc.in_features, num_classes)\n",
    "        self.fc_kpts = nn.Linear(self.resnet50.fc.in_features, self.num_keypoints * 2)\n",
    "        self.resnet50.fc = nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.resnet50(x)\n",
    "        class_outputs = self.fc_cls(features)\n",
    "        keypoint_outputs = self.fc_kpts(features)\n",
    "        return class_outputs, keypoint_outputs.reshape(-1, self.num_keypoints, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205d9968-f693-4475-bca7-d310175d17a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scipy hadamard  (32, 32)\n",
      "Replaced  (32, 32)\n"
     ]
    }
   ],
   "source": [
    "class_config_path = './../config/formated_class.yaml'\n",
    "DATASET_FOLDER_EVAL = './../../../RnD_datasets/robocup_dataset/evaluation'\n",
    "\n",
    "num_classes = len(load_class_info(class_config_path))\n",
    "codewords = generate_ecc_codes(num_classes, 16)\n",
    "\n",
    "fusion_types = ['concat','add','multiply']\n",
    "batch_size = 1\n",
    "\n",
    "eval_dataset_ecc= CustomDataLoaderECC(DATASET_FOLDER_EVAL, class_config_path, 'concat', transform=transform, label_transform=label_transform_ecc, codeWords=codewords)\n",
    "dataloader_ecc = DataLoader(eval_dataset_ecc, batch_size=batch_size, shuffle=False)\n",
    "# print(len(dataloader_ecc))\n",
    "eval_dataset_onehot = CustomDataLoaderOneHot(DATASET_FOLDER_EVAL, class_config_path, transform=transform, label_transform=label_transform, fusion_type='concat')\n",
    "dataloader_onehot = DataLoader(eval_dataset_onehot, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "eval_dataset_normal = CustomDataLoaderNormal(DATASET_FOLDER_EVAL, class_config_path, transform=transform, demo=True)\n",
    "dataloader_normal = DataLoader(eval_dataset_normal, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79bd9631-c749-4d26-a73c-8ccb6e89c995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "model_18_ecc = CustomResNet18(18, 1, 4)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "model_18_ecc.load_state_dict(torch.load('./resnet18/models/updated_models/error_correcting/resnet18_conditional_concat_100_epochs_ecc.pth'))\n",
    "model_18_ecc.eval()\n",
    "\n",
    "print('model loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73655843-66f5-41e1-9864-67b59a79c300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time: 12.0755 milli seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "inference_times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets, label in dataloader_ecc:\n",
    "        # print(images.shape)\n",
    "        start_time = time.time()\n",
    "        class_outputs, keypoint_outputs = model_18_ecc(images)\n",
    "        end_time = time.time()\n",
    "        # print(targets['labels'])\n",
    "        # print(predictions)\n",
    "        inference_time = (end_time - start_time) * 1000\n",
    "        inference_times.append(inference_time)\n",
    "        # print(f'Inference time: {inference_time:.2f} ms')\n",
    "        results.append((keypoint_outputs, targets['keypoints']))\n",
    "\n",
    "# print(np.average(inference_times))\n",
    "average_inference_time = sum(inference_times) / len(inference_times)\n",
    "print(f\"Average inference time: {average_inference_time:.4f} milli seconds\")\n",
    "type(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "126d5e7a-a0c8-49c7-9562-abd7130d7798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_oks(pred_keypoints, gt_keypoints, scale=224, sigma=0.026):\n",
    "    \n",
    "    pred_keypoints = np.array(pred_keypoints).squeeze()\n",
    "    gt_keypoints = np.array(gt_keypoints).squeeze()\n",
    "    \n",
    "    if pred_keypoints.ndim == 1:\n",
    "        pred_keypoints = pred_keypoints.reshape(1, -1)\n",
    "    if gt_keypoints.ndim == 1:\n",
    "        gt_keypoints = gt_keypoints.reshape(1, -1)\n",
    "    \n",
    "    dx = pred_keypoints[:, 0] - gt_keypoints[:, 0]\n",
    "    dy = pred_keypoints[:, 1] - gt_keypoints[:, 1]\n",
    "    \n",
    "    distances = np.sqrt(dx**2 + dy**2)\n",
    "    # scale = np.sqrt(2) * image_size\n",
    "    # print(scale)\n",
    "    \n",
    "    normalized_distances = distances / scale\n",
    "    oks = np.mean(np.exp(-normalized_distances**2 / (2 * sigma**2)))\n",
    "    return oks\n",
    "\n",
    "def calculate_precision_recall(oks_scores, gt_keypoints, thresholds):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        tp = np.sum(oks_scores >= threshold)\n",
    "        fp = np.sum(oks_scores < threshold)\n",
    "        fn = len(gt_keypoints) - tp\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    return precisions, recalls\n",
    "\n",
    "def calculate_ap(precisions, recalls):\n",
    "\n",
    "    precisions = np.array(precisions)\n",
    "    recalls = np.array(recalls)\n",
    "    \n",
    "    # Sort by recall\n",
    "    sorted_indices = np.argsort(recalls)\n",
    "    precisions = precisions[sorted_indices]\n",
    "    recalls = recalls[sorted_indices]\n",
    "    \n",
    "    # Compute AP\n",
    "    ap = np.trapz(precisions, recalls)\n",
    "    return ap\n",
    "\n",
    "def calculate_map(predictions, ground_truths, scale=224, sigma=0.026, thresholds=np.linspace(0.5, 0.95, 10)):\n",
    "    \n",
    "    all_aps = []\n",
    "    \n",
    "    for pred_keypoints, gt_keypoints in zip(predictions, ground_truths):\n",
    "        oks_scores = []\n",
    "        \n",
    "        for pred_kpts, gt_kpts in zip(pred_keypoints, gt_keypoints):\n",
    "            oks = calculate_oks(pred_kpts, gt_kpts, scale=scale, sigma=sigma)\n",
    "            oks_scores.append(oks)\n",
    "        \n",
    "        precisions, recalls = calculate_precision_recall(oks_scores, gt_keypoints, thresholds)\n",
    "        ap = calculate_ap(precisions, recalls)\n",
    "        all_aps.append(ap)\n",
    "    \n",
    "    mAP = np.mean(all_aps)\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af307c52-2b12-4498-b957-77ee60119b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object Keypoint Similarity (OKS): 0.0145\n",
      "Mean Average Precision (mAP): 0.0067\n"
     ]
    }
   ],
   "source": [
    "num_instances = 0\n",
    "total_oks = 0\n",
    "total_mAp = 0\n",
    "image_size = 224\n",
    "\n",
    "for predictions, targets in results:\n",
    "    # Ensure pred_kpts and target_kpts are numpy arrays\n",
    "    pred_kpts = predictions.numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "    target_kpts = targets.numpy() if isinstance(targets, torch.Tensor) else targets\n",
    "    \n",
    "    # Calculate scale factor for each instance\n",
    "    scale_factor = np.sqrt(2) * image_size\n",
    "    \n",
    "    oks = calculate_oks(pred_kpts, target_kpts, scale_factor)\n",
    "    mAp = calculate_map(pred_kpts, target_kpts, scale_factor)\n",
    "\n",
    "    total_mAp += mAp\n",
    "    total_oks += oks\n",
    "    num_instances += 1\n",
    "\n",
    "average_oks = total_oks / num_instances\n",
    "average_mAp = total_mAp / num_instances\n",
    "print(f\"Object Keypoint Similarity (OKS): {average_oks:.4f}\")\n",
    "print(f\"Mean Average Precision (mAP): {average_mAp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03f78dd6-4737-4e1a-9da0-1e801a32c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_speed_inference(dataloader, model_path, custom_model, encoding_type, model_name, fusion, epoch):\n",
    "    results = []\n",
    "    inference_times = []\n",
    "    \n",
    "    if fusion != 'concat':\n",
    "        model = custom_model(18, 1, 3)\n",
    "    else:\n",
    "        model = custom_model(18, 1, 4)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # model.to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    print('model loaded successfully')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets, label in dataloader:\n",
    "            # images.to(device)\n",
    "            # print(images.shape)\n",
    "            start_time = time.time()\n",
    "            class_outputs, keypoint_outputs = model(images)\n",
    "            end_time = time.time()\n",
    "            inference_time = (end_time - start_time) * 1000\n",
    "            inference_times.append(inference_time)\n",
    "            results.append((keypoint_outputs, targets['keypoints']))\n",
    "    \n",
    "    average_inference_time = sum(inference_times) / len(inference_times)\n",
    "    print(f\"Average inference time for {model_name} - {encoding_type} - {fusion} - {epoch} : {average_inference_time:.4f} milli seconds\")\n",
    "    # print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63c90435-dbf2-4909-b236-1fb9fc227303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****resnet18******\n",
      "*****error_correcting******\n",
      "\n",
      "*****concat******\n",
      "\n",
      "./resnet18/models/updated_models/error_correcting/resnet18_conditional_concat_100_epochs_ecc.pth\n",
      "model loaded successfully\n",
      "Average inference time for resnet18 - error_correcting - concat - 100 : 13.0408 milli seconds\n",
      "Object Keypoint Similarity (OKS): 0.0145\n",
      "Mean Average Precision (mAP): 0.0067 \n",
      "\n",
      "./resnet18/models/updated_models/error_correcting/resnet18_conditional_concat_250_epochs_ecc.pth\n",
      "model loaded successfully\n",
      "Average inference time for resnet18 - error_correcting - concat - 250 : 10.9500 milli seconds\n",
      "Object Keypoint Similarity (OKS): 0.0107\n",
      "Mean Average Precision (mAP): 0.0044 \n",
      "\n",
      "\n",
      "*****add******\n",
      "\n",
      "./resnet18/models/updated_models/error_correcting/resnet18_conditional_add_100_epochs_ecc.pth\n",
      "model loaded successfully\n",
      "Average inference time for resnet18 - error_correcting - add - 100 : 10.5539 milli seconds\n",
      "Object Keypoint Similarity (OKS): 0.0078\n",
      "Mean Average Precision (mAP): 0.0030 \n",
      "\n",
      "./resnet18/models/updated_models/error_correcting/resnet18_conditional_add_250_epochs_ecc.pth\n",
      "model loaded successfully\n",
      "Average inference time for resnet18 - error_correcting - add - 250 : 11.2905 milli seconds\n",
      "Object Keypoint Similarity (OKS): 0.0062\n",
      "Mean Average Precision (mAP): 0.0022 \n",
      "\n",
      "\n",
      "*****multiply******\n",
      "\n",
      "./resnet18/models/updated_models/error_correcting/resnet18_conditional_multiply_100_epochs_ecc.pth\n",
      "model loaded successfully\n",
      "Average inference time for resnet18 - error_correcting - multiply - 100 : 9.8673 milli seconds\n",
      "Object Keypoint Similarity (OKS): 0.0067\n",
      "Mean Average Precision (mAP): 0.0022 \n",
      "\n",
      "./resnet18/models/updated_models/error_correcting/resnet18_conditional_multiply_250_epochs_ecc.pth\n",
      "model loaded successfully\n",
      "Average inference time for resnet18 - error_correcting - multiply - 250 : 10.4958 milli seconds\n",
      "Object Keypoint Similarity (OKS): 0.0059\n",
      "Mean Average Precision (mAP): 0.0019 \n",
      "\n",
      "*****one_hot******\n",
      "\n",
      "*****concat******\n",
      "\n",
      "./resnet18/models/updated_models/one_hot/resnet18_conditional_concat_100_epochs_onehot.pth\n",
      "model loaded successfully\n",
      "Average inference time for resnet18 - one_hot - concat - 100 : 11.2464 milli seconds\n",
      "Object Keypoint Similarity (OKS): 0.0062\n",
      "Mean Average Precision (mAP): 0.0022 \n",
      "\n",
      "./resnet18/models/updated_models/one_hot/resnet18_conditional_concat_250_epochs_onehot.pth\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './resnet18/models/updated_models/one_hot/resnet18_conditional_concat_250_epochs_onehot.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(path)\n\u001b[1;32m     40\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(eval_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 41\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mavg_speed_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfusion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m predictions, targets \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Ensure pred_kpts and target_kpts are numpy arrays\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     pred_kpts \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(predictions, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m predictions\n",
      "Cell \u001b[0;32mIn[25], line 11\u001b[0m, in \u001b[0;36mavg_speed_inference\u001b[0;34m(dataloader, model_path, custom_model, encoding_type, model_name, fusion, epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# model.to(device)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel loaded successfully\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './resnet18/models/updated_models/one_hot/resnet18_conditional_concat_250_epochs_onehot.pth'"
     ]
    }
   ],
   "source": [
    "dataloaders_list = [dataloader_ecc, dataloader_onehot, dataloader_normal]\n",
    "epochs_list = [100, 250]\n",
    "fusion_types = ['concat','add','multiply']\n",
    "custom_models = [CustomResNet18, CustomResNet50]\n",
    "model_names = ['resnet18', 'resnet50'] #add SSD\n",
    "encoding_types = ['error_correcting','one_hot']\n",
    "enc_shorts = ['ecc', 'onehot']\n",
    "# encoding_types = ['ecc', 'onehot', 'normal']\n",
    "batch_size = 1\n",
    "\n",
    "# './'+self.model_name+'/models/updated_models/error_correcting/resnet18_conditional_'+fusion+'_'+str(epochs)+'_epochs_ecc.pth'\n",
    "path = None\n",
    "num_instances = 0\n",
    "total_oks = 0\n",
    "total_mAp = 0\n",
    "image_size = 224\n",
    "\n",
    "for model_name in model_names:\n",
    "    print('*****'+model_name+'******')\n",
    "    for custom_model in custom_models:\n",
    "        # print('*****'+custom_model+'******')\n",
    "        for encoding_type in encoding_types:\n",
    "            print('*****'+encoding_type+'******')\n",
    "            for fusion in fusion_types:\n",
    "                print('\\n*****'+fusion+'******\\n')\n",
    "                for epoch in epochs_list:\n",
    "                    results = []\n",
    "                    eval_dataset = None\n",
    "                    if(encoding_type=='error_correcting'):\n",
    "                        eval_dataset = CustomDataLoaderECC(DATASET_FOLDER_EVAL, class_config_path, fusion, transform=transform, \n",
    "                                                           label_transform=label_transform_ecc, codeWords=codewords)\n",
    "                        path = './'+model_name+'/models/updated_models/'+encoding_type+'/'+model_name+'_conditional_'+fusion+'_'+str(epoch)+'_epochs_ecc.pth'\n",
    "                    \n",
    "                    elif(encoding_type=='one_hot'):\n",
    "                        eval_dataset = CustomDataLoaderECC(DATASET_FOLDER_EVAL, class_config_path, fusion, transform=transform, \n",
    "                                                           label_transform=label_transform_ecc, codeWords=codewords)\n",
    "                        path = './'+model_name+'/models/updated_models/'+encoding_type+'/'+model_name+'_conditional_'+fusion+'_'+str(epoch)+'_epochs_onehot.pth'\n",
    "\n",
    "                    print(path)\n",
    "                    dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "                    results = avg_speed_inference(dataloader, path, custom_model, encoding_type, model_name, fusion, epoch)\n",
    "                    for predictions, targets in results:\n",
    "                        # Ensure pred_kpts and target_kpts are numpy arrays\n",
    "                        pred_kpts = predictions.numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "                        target_kpts = targets.numpy() if isinstance(targets, torch.Tensor) else targets\n",
    "                        \n",
    "                        # Calculate scale factor for each instance\n",
    "                        scale_factor = np.sqrt(2) * image_size\n",
    "                        \n",
    "                        oks = calculate_oks(pred_kpts, target_kpts, scale_factor)\n",
    "                        mAp = calculate_map(pred_kpts, target_kpts, scale_factor)\n",
    "                    \n",
    "                        total_mAp += mAp\n",
    "                        total_oks += oks\n",
    "                        num_instances += 1\n",
    "\n",
    "                    average_oks = total_oks / num_instances\n",
    "                    average_mAp = total_mAp / num_instances\n",
    "                    print(f\"Object Keypoint Similarity (OKS): {average_oks:.4f}\")\n",
    "                    print(f\"Mean Average Precision (mAP): {average_mAp:.4f} \\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4eeb9b-764d-47e0-bd2f-7f6678e7bc63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
